import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
import os
import argparse
import optuna
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss

# --- 1. å‚æ•°é…ç½® ---
parser = argparse.ArgumentParser()
parser.add_argument("--use_deberta", action="store_true", help="Include DeBERTa OOF as features")
parser.add_argument("--use_qwen", action="store_true", help="Include Qwen OOF as features")
parser.add_argument("--use_manual_feats", action="store_true", help="Include manual engineered features")
parser.add_argument("--tune", action="store_true", help="Run Optuna hyperparameter tuning")
parser.add_argument("--n_trials", type=int, default=50, help="Number of Optuna trials")
args = parser.parse_args()

# --- 2. æ–‡ä»¶è·¯å¾„ ---
FEATURE_PATH = "data/processed/train_features_structured.parquet"
DEBERTA_OOF_PATH = "data/processed/oof_deberta_v3_large.csv"
QWEN_OOF_PATH = "data/processed/oof_qwen_14b.csv"
OUTPUT_DIR = "outputs/stacking_models"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- 3. æ•°æ®åŠ è½½ä¸Žç‰¹å¾æ‹¼æŽ¥ ---
def load_data():
    print("ðŸ”„ Loading and preparing data...")
    
    # [A] åŸºç¡€æ•°æ® (æ‰‹å·¥ç‰¹å¾ + Target)
    df = pd.read_parquet(FEATURE_PATH)
    
    df['target'] = df[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1).map({
        'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2
    })
    
    # [B] æ‰‹å·¥ç‰¹å¾
    # åˆ æŽ‰æ‰€æœ‰æ ‡ç­¾åˆ—ï¼Œå‰©ä¸‹çš„å°±æ˜¯ç‰¹å¾
    drop_cols = ['winner_model_a', 'winner_model_b', 'winner_tie', 'target']
    manual_feats = [c for c in df.columns if c not in drop_cols]
    
    # --- [å…³é”®ä¿®æ”¹] ä½¿ç”¨ concat ä»£æ›¿ merge ---
    
    # [C] LLM OOF ç‰¹å¾
    llm_feats = []
    data_to_concat = [df] # å‡†å¤‡ä¸€ä¸ªåˆ—è¡¨å­˜æ”¾è¦æ‹¼æŽ¥çš„ df

    if args.use_deberta:
        print("   -> Merging DeBERTa OOF...")
        oof_deb = pd.read_csv(DEBERTA_OOF_PATH)
        # é‡å‘½å
        oof_deb = oof_deb[['pred_a', 'pred_b', 'pred_tie']].rename(columns={
            'pred_a': 'deberta_a', 'pred_b': 'deberta_b', 'pred_tie': 'deberta_tie'
        })
        data_to_concat.append(oof_deb)
        llm_feats.extend(['deberta_a', 'deberta_b', 'deberta_tie'])

    if args.use_qwen:
        print("   -> Merging Qwen OOF...")
        oof_qwen = pd.read_csv(QWEN_OOF_PATH)
        oof_qwen = oof_qwen[['pred_a', 'pred_b', 'pred_tie']].rename(columns={
            'pred_a': 'qwen_a', 'pred_b': 'qwen_b', 'pred_tie': 'qwen_tie'
        })
        data_to_concat.append(oof_qwen)
        llm_feats.extend(['qwen_a', 'qwen_b', 'qwen_tie'])
    
    # æ¨ªå‘æ‹¼æŽ¥æ‰€æœ‰ DataFrame
    df = pd.concat(data_to_concat, axis=1)

    # [D] ç¡®å®šæœ€ç»ˆç‰¹å¾åˆ—è¡¨
    final_features = []
    if args.use_manual_feats:
        final_features.extend(manual_feats)
    if llm_feats:
        final_features.extend(llm_feats)
        
    if not final_features:
        raise ValueError("No features selected!")
        
    df = df.replace([np.inf, -np.inf], np.nan)
    
    print(f"âœ… Training with {len(final_features)} features.")
    return df, final_features

# --- 4. è®­ç»ƒ/è°ƒä¼˜é€»è¾‘ ---

def train_lgbm(params, X, y, features):
    """è®­ç»ƒ LightGBM å¹¶è¿”å›ž CV åˆ†æ•°å’Œæ¨¡åž‹"""
    print("\nðŸš€ Starting LightGBM Training...")
    
    # å¤„ç†ç±»åˆ«ç‰¹å¾
    cat_feats = [c for c in features if 'cluster' in c or 'is_code' in c]
    for c in cat_feats:
        X[c] = X[c].astype('category')

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    oof_preds = np.zeros((len(X), 3))
    models = []
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]
        
        dtrain = lgb.Dataset(X_train[features], label=y_train)
        dval = lgb.Dataset(X_val[features], label=y_val)
        
        model = lgb.train(
            params, dtrain, 
            num_boost_round=1000,
            valid_sets=[dval],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
        oof_preds[val_idx] = model.predict(X_val[features])
        models.append(model)
    
    score = log_loss(y, oof_preds)
    print(f"ðŸ† LightGBM CV Score: {score:.5f}")
    return models, score

def main():
    df, features = load_data()
    X = df[features]
    y = df['target']
    
    # å›ºå®šçš„æœ€ä½³å‚æ•° (ä»Žä¹‹å‰çš„ Optuna èŽ·å¾—)
    best_lgbm_params = {
        'objective': 'multiclass', 'num_class': 3, 'metric': 'multi_logloss',
        'verbosity': -1, 'seed': 42, 'learning_rate': 0.0169, 
        'num_leaves': 32, 'max_depth': 9, 'feature_fraction': 0.67, 
        'bagging_fraction': 0.88, 'bagging_freq': 6, 
        'min_child_samples': 16, 'lambda_l1': 0.0001, 'lambda_l2': 0.006
    }
    
    models, _ = train_lgbm(best_lgbm_params, X, y, features)
    
    # ä¿å­˜æ¨¡åž‹
    print("\nðŸ’¾ Saving LGBM models...")
    exp_name = ""
    if args.use_deberta: exp_name += "deb_"
    if args.use_qwen: exp_name += "qwen_"
    if args.use_manual_feats: exp_name += "feats"
    
    for i, model in enumerate(models):
        model.save_model(f"{OUTPUT_DIR}/lgbm_{exp_name}_fold{i}.txt")
    print(f"   -> Models saved with prefix: lgbm_{exp_name}")

if __name__ == "__main__":
    main()