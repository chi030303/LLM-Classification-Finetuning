import os
import argparse
import pandas as pd
import numpy as np
from sklearn.metrics import log_loss, accuracy_score
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorWithPadding,
    EarlyStoppingCallback
)
from datasets import Dataset

# --- 1. å‚æ•°è§£æ (å·²å¯¹é½è·¯å¾„) ---
parser = argparse.ArgumentParser()
parser.add_argument("--fold", type=int, default=0, help="Run training on which fold")
# è¿™é‡Œ model_name ä¾ç„¶ä¼ æ¨¡å‹IDï¼Œè·¯å¾„é€šè¿‡ HF_HOME ç¯å¢ƒå˜é‡æ§åˆ¶
parser.add_argument("--model_name", type=str, default="microsoft/deberta-v3-large") 
parser.add_argument("--data_path", type=str, default="data/processed/train_with_folds.csv") # å¯¹é½æ•°æ®è·¯å¾„
parser.add_argument("--output_dir", type=str, default="outputs/models/deberta_v3_large")   # å¯¹é½è¾“å‡ºè·¯å¾„
parser.add_argument("--resume_from_checkpoint", type=str, default=None, help="Path to checkpoint to resume from")
parser.add_argument("--max_len", type=int, default=1536, help="Max sequence length") 

parser.add_argument("--per_device_train_batch_size", type=int, default=2)
parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
parser.add_argument("--num_train_epochs", type=int, default=3)
parser.add_argument("--learning_rate", type=float, default=5e-6)
parser.add_argument("--dataloader_num_workers", type=int, default=4)
# Boolean å¼€å…³
parser.add_argument("--gradient_checkpointing", action="store_true", help="Enable gradient checkpointing")
parser.add_argument("--bf16", action="store_true", help="Enable bf16")
parser.add_argument("--fp16", action="store_true", help="Enable fp16")

args = parser.parse_args()

REAL_OUTPUT_DIR = f"{args.output_dir}_fold{args.fold}"
print(f"ğŸ¯ Real Output Directory: {REAL_OUTPUT_DIR}")
# --- 2. æ•°æ®å¤„ç† ---
tokenizer = AutoTokenizer.from_pretrained(args.model_name)

def preprocess_function(examples):
    # DeBERTa å¤„ç† 1024 é•¿åº¦é€šå¸¸å ç”¨çº¦ 12G-15G æ˜¾å­˜ (BS=4)
    prompts = [str(x) if x is not None else "" for x in examples["prompt_text"]]
    res_a = [str(x) if x is not None else "" for x in examples["res_a_text"]]
    res_b = [str(x) if x is not None else "" for x in examples["res_b_text"]]

    sep = tokenizer.sep_token 
    combined_texts = [
        f"{p} {sep} {a} {sep} {b}" 
        for p, a, b in zip(prompts, res_a, res_b)
    ]
    
    # 3. ä¼ ç»™ Tokenizer (ç°åœ¨åªä¼ ä¸€ä¸ªåˆ—è¡¨)
    return tokenizer(
        combined_texts,
        truncation=True,
        max_length=args.max_len, 
        padding=False # DataCollator ä¼šè´Ÿè´£åŠ¨æ€ Paddingï¼Œè¿™é‡Œ False æ˜¯å¯¹çš„
    )

# è¯»å– CSV (å¦‚æœæ˜¯ parquet æ”¹ä¸º pd.read_parquet)
df = pd.read_csv(args.data_path) 

# Label è½¬æ¢
df['label'] = df[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1).map({
    'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2
})

train_df = df[df['fold'] != args.fold].reset_index(drop=True)
valid_df = df[df['fold'] == args.fold].reset_index(drop=True)

train_ds = Dataset.from_pandas(train_df)
valid_ds = Dataset.from_pandas(valid_df)

train_ds = train_ds.map(preprocess_function, batched=True)
valid_ds = valid_ds.map(preprocess_function, batched=True)

# --- 3. æ¨¡å‹ ---
model = AutoModelForSequenceClassification.from_pretrained(
    args.model_name, 
    num_labels=3,
    attention_probs_dropout_prob=0.1,
    hidden_dropout_prob=0.1
)

# --- 4. è®­ç»ƒå‚æ•° (å•å¡ V100 è°ƒä¼˜) ---
training_args = TrainingArguments(
    output_dir=REAL_OUTPUT_DIR,
    learning_rate=args.learning_rate,             # å•å¡é€šå¸¸è°ƒä½ä¸€ç‚¹ LR æ¯”è¾ƒç¨³
    per_device_train_batch_size=args.per_device_train_batch_size,  # 48G æ˜¾å­˜è·‘ 1536 é•¿åº¦ï¼ŒBS=4 åº”è¯¥å¾ˆè½»æ¾
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=args.gradient_accumulation_steps,  # 4 * 4 = 16 (ç­‰æ•ˆ Batch Size)
    num_train_epochs=args.num_train_epochs,
    weight_decay=0.01,
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="log_loss",
    greater_is_better=False,
    bf16=args.bf16,
    fp16=args.fp16,
    report_to="none",
    disable_tqdm=True,
    gradient_checkpointing=args.gradient_checkpointing,
    dataloader_num_workers=args.dataloader_num_workers,
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()
    loss = log_loss(labels, probs)
    acc = accuracy_score(labels, np.argmax(probs, axis=1))
    return {"log_loss": loss, "accuracy": acc}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=valid_ds,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],
)

last_checkpoint = None
if args.resume_from_checkpoint:
    print(f"ğŸ›‘ Manual Resume: Forcing resume from {args.resume_from_checkpoint}")
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)

# å¦åˆ™å°è¯•è‡ªåŠ¨å¯»æ‰¾
elif os.path.isdir(REAL_OUTPUT_DIR):
    from transformers.trainer_utils import get_last_checkpoint
    last_checkpoint = get_last_checkpoint(REAL_OUTPUT_DIR)
    if last_checkpoint is not None:
        print(f"ğŸ”„ Auto Resume: Found checkpoint {last_checkpoint}")
        trainer.train(resume_from_checkpoint=True)
    else:
        print("ğŸš€ No checkpoint found. Starting new training run...")
        trainer.train()
# å…¨æ–°è®­ç»ƒ
else:
    print("ğŸš€ New output directory. Starting new training run...")
    trainer.train()