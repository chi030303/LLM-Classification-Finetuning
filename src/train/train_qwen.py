import os
# [å…³é”®] è§£å†³ Tokenizer æ­»é”è­¦å‘Šï¼Œå¿…é¡»æ”¾åœ¨æœ€å‰é¢
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import argparse
import pandas as pd
import numpy as np
import torch
import textwrap
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer,
    BitsAndBytesConfig,
    DataCollatorWithPadding
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from sklearn.metrics import log_loss, accuracy_score

# --- 1. å‚æ•°è®¾ç½® ---
parser = argparse.ArgumentParser()
parser.add_argument("--fold", type=int, default=0)
# ä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„ (Qwen2.5/Qwen3)
parser.add_argument("--model_name", type=str, default="/root/autodl-tmp/base_models/Qwen3-14") 
parser.add_argument("--data_path", type=str, default="data/processed/train_with_folds.csv")
parser.add_argument("--output_dir", type=str, default="outputs/models/qwen_14b")
parser.add_argument("--max_len", type=int, default=2048) # ä¿æŒä½ è¦æ±‚çš„é•¿åº¦
parser.add_argument("--learning_rate", type=float, default=2e-4)
args = parser.parse_args()

REAL_OUTPUT_DIR = f"{args.output_dir}_fold{args.fold}"
print(f"ğŸš€ Training Fold {args.fold} | Model: {args.model_name}")

# --- 2. å¢å¼ºç‰ˆ Prompt (ChatML æ ¼å¼) ---
def preprocess_function(examples):
    inputs = []
    # ä½¿ç”¨æ¸…æ´—è¿‡çš„åˆ—
    for p, a, b in zip(examples["prompt_text"], examples["res_a_text"], examples["res_b_text"]):
        # ä½¿ç”¨ textwrap.dedent å»é™¤ä»£ç ç¼©è¿›ï¼Œç¡®ä¿ Prompt å¹²å‡€
        text = textwrap.dedent(f"""\
            <|im_start|>system
            You are a helpful assistant acting as a judge. Please evaluate which response is better.<|im_end|>
            <|im_start|>user
            Evaluate the two responses to the user question.

            Question:
            {p}

            Candidate Response A:
            {a}

            Candidate Response B:
            {b}

            You must choose the preferred answer.
            Respond strictly with one of: A, B, Tie.
            Do not provide any explanation.<|im_end|>
            <|im_start|>assistant
            """)
        inputs.append(text)
    
    return tokenizer(inputs, truncation=True, max_length=args.max_len, padding=False)

# --- 3. åŠ è½½æ•°æ® ---
# å¡«å……ç©ºå€¼é˜²æ­¢æŠ¥é”™
df = pd.read_csv(args.data_path).fillna("")
# Label Mapping
df['label'] = df[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1).map({
    'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2
})

train_ds = Dataset.from_pandas(df[df['fold'] != args.fold])
valid_ds = Dataset.from_pandas(df[df['fold'] == args.fold])

# --- 4. æ¨¡å‹ä¸ Tokenizer ---
# 4-bit é‡åŒ–é…ç½® (QLoRA)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16, # RTX 6000 å¿…é¡»å¼€ BF16
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained(args.model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right' # åˆ†ç±»ä»»åŠ¡å¿…é¡»å³å¡«å……

model = AutoModelForSequenceClassification.from_pretrained(
    args.model_name,
    num_labels=3,
    quantization_config=bnb_config,
    attn_implementation="flash_attention_2", # ğŸ”¥ å…³é”®åŠ é€Ÿï¼šå¼€å¯ Flash Attention 2
    device_map="auto",
    torch_dtype=torch.bfloat16
)

model = prepare_model_for_kbit_training(model)

# LoRA é…ç½®
peft_config = LoraConfig(
    r=16, 
    lora_alpha=32, 
    lora_dropout=0.05,
    # Qwen å…¨æ¨¡å—å¾®è°ƒæ•ˆæœæœ€å¥½
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], 
    task_type=TaskType.SEQ_CLS,
    modules_to_save=["score"] # å¿…é¡»è®­ç»ƒåˆ†ç±»å¤´
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
model.config.pad_token_id = tokenizer.pad_token_id 

# --- 5. è®­ç»ƒå‚æ•° (å·²ä¼˜åŒ–é¢‘ç‡) ---
train_ds = train_ds.map(preprocess_function, batched=True)
valid_ds = valid_ds.map(preprocess_function, batched=True)

training_args = TrainingArguments(
    output_dir=REAL_OUTPUT_DIR,
    learning_rate=args.learning_rate,
    
    # [æ˜¾å­˜ä¼˜åŒ–] RTX 6000 96G æ˜¾å­˜å·¨å¤§ï¼ŒBS å¼€å¤§æé€Ÿ
    # per_device_train_batch_size=16,   
    per_device_train_batch_size=8,  
    gradient_accumulation_steps=4,  # æ€» Batch è¿˜æ˜¯ 32
    per_device_eval_batch_size=16,
    
    num_train_epochs=1,               # 14B è·‘ 1 è½®
    bf16=True,                        # å¿…é¡»å¼€å¯ BF16
    
    logging_steps=10,
    
    # [è¯„ä¼°é¢‘ç‡ä¼˜åŒ–] å‡å°‘è¯„ä¼°æ¬¡æ•°ï¼Œä¸“æ³¨è®­ç»ƒ
    # å‡è®¾æ•°æ®é‡ 4.6w, Batch 32 -> epoch steps â‰ˆ 1437
    # è®¾ä¸º 0.2 è¡¨ç¤ºæ¯è·‘ 20% (çº¦280æ­¥) è¯„ä¼°ä¸€æ¬¡
    eval_strategy="steps",
    eval_steps=300,                   
    save_strategy="steps",
    save_steps=300,
    save_total_limit=2,               # åªç•™æœ€è¿‘2ä¸ª
    
    load_best_model_at_end=True,
    metric_for_best_model="log_loss",
    greater_is_better=False,
    
    gradient_checkpointing=True,      # æ˜¾å­˜ä¼˜åŒ–
    max_grad_norm=1.0,                # é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    warmup_ratio=0.05,                
    
    report_to="none",
    disable_tqdm=True,
    dataloader_num_workers=4          
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple): logits = logits[0]
    # [å…³é”®ä¿®å¤] å¿…é¡»ç”¨ numpy ä¸”é˜²æ­¢æº¢å‡º
    # ä½¿ç”¨ float32 ç¡®ä¿ç²¾åº¦
    probs = torch.nn.functional.softmax(torch.tensor(logits, dtype=torch.float32), dim=-1).numpy()
    
    return {
        "log_loss": log_loss(labels, probs), 
        "accuracy": accuracy_score(labels, np.argmax(probs, axis=1))
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=valid_ds,
    # tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
)

print("ğŸš€ Starting Qwen Training...")
trainer.train()

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
trainer.save_model(REAL_OUTPUT_DIR)
print(f"âœ… Training Complete. Model saved to {REAL_OUTPUT_DIR}")