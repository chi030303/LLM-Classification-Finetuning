import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
import joblib
import os
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss

# --- é…ç½® ---
FEATURE_PATH = "data/processed/train_features_structured.parquet"
OOF_PATHS = {
    "deberta": "data/processed/oof_deberta_v3_large.csv",
    # "qwen": "data/processed/oof_qwen_14b.csv" # å¦‚æœæœ‰Qwenå°±è§£å¼€æ³¨é‡Š
}
OUTPUT_DIR = "outputs/stacking_models"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- 1. åŠ è½½å¹¶åˆå¹¶æ•°æ® ---
print("ğŸ”„ Loading Data...")
# åŠ è½½æ‰‹å·¥ç‰¹å¾
df = pd.read_parquet(FEATURE_PATH)

# æ£€æŸ¥å¹¶åˆ é™¤ cluster_id
if 'cluster_id' in df.columns:
    print(f"âš ï¸ Found redundant column 'cluster_id'. Removing...")
    df = df.drop(columns=['cluster_id'])
    
    # è¦†ç›–ä¿å­˜
    df.to_parquet(FEATURE_PATH, index=False)
    print(f"âœ… Saved cleaned dataframe to {FEATURE_PATH}")
    print(f"Current columns: {[c for c in df.columns if 'cluster' in c]}")
else:
    print("âœ… 'cluster_id' not found. Data is already clean.")

# åŠ è½½ LLM OOF å¹¶åˆå¹¶
for model_name, path in OOF_PATHS.items():
    if os.path.exists(path):
        print(f"   -> Merging {model_name} OOF...")
        oof = pd.read_csv(path)
        # å‡è®¾è¡Œé¡ºåºä¸€è‡´ï¼ˆé€šå¸¸æ˜¯ä¸€è‡´çš„ï¼‰ï¼Œç›´æ¥èµ‹å€¼
        df[f'{model_name}_a'] = oof['pred_a']
        df[f'{model_name}_b'] = oof['pred_b']
        df[f'{model_name}_tie'] = oof['pred_tie']
    else:
        print(f"âš ï¸ Warning: {path} not found, skipping...")

# å‡†å¤‡ Target
df['target'] = df[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1).map({
    'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2
})

# å‡†å¤‡ç‰¹å¾åˆ—è¡¨ (æ’é™¤ ID, Label ç­‰éç‰¹å¾åˆ—)
drop_cols = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie', 'fold', 'target', 
             'prompt_text', 'res_a_text', 'res_b_text'] # ç¡®ä¿æ’é™¤æ–‡æœ¬åˆ—
features = [c for c in df.columns if c not in drop_cols]
print(f"âœ… Training on {len(features)} features.")

# --- 2. è®­ç»ƒ LightGBM ---
print("\nğŸš€ Starting LightGBM Training...")
lgb_params = {
    'objective': 'multiclass',
    'num_class': 3,
    'metric': 'multi_logloss',
    'verbosity': -1,
    'seed': 42,
    'learning_rate': 0.0169, 
    'num_leaves': 32, 
    'max_depth': 9, 
    'feature_fraction': 0.67, 
    'bagging_fraction': 0.88, 
    'bagging_freq': 6, 
    'min_child_samples': 16, 
    'lambda_l1': 0.0001, 
    'lambda_l2': 0.006
}

# è½¬æ¢ Categorical ç‰¹å¾ (Cluster ID)
cat_feats = [c for c in features if 'cluster' in c or 'is_code' in c]
for c in cat_feats:
    df[c] = df[c].astype('category')

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
lgb_oof = np.zeros((len(df), 3))

for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target'])):
    X_train, y_train = df.iloc[train_idx][features], df.iloc[train_idx]['target']
    X_val, y_val = df.iloc[val_idx][features], df.iloc[val_idx]['target']
    
    dtrain = lgb.Dataset(X_train, label=y_train)
    dval = lgb.Dataset(X_val, label=y_val)
    
    model = lgb.train(
        lgb_params, dtrain, 
        num_boost_round=1000,
        valid_sets=[dval],
        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)] # 0 è¡¨ç¤ºä¸æ‰“å°åˆ·å±
    )
    
    lgb_oof[val_idx] = model.predict(X_val)
    # ä¿å­˜æ¨¡å‹
    model.save_model(f"{OUTPUT_DIR}/lgbm_fold{fold}.txt")
    print(f"   LGBM Fold {fold} LogLoss: {log_loss(y_val, lgb_oof[val_idx]):.5f}")

print(f"ğŸ† Final LightGBM CV Score: {log_loss(df['target'], lgb_oof):.5f}")

# --- 3. è®­ç»ƒ XGBoost ---
print("\nğŸš€ Starting XGBoost Training...")
# XGBoost éœ€è¦å°† category è½¬å› int æˆ– enable_categorical
for c in cat_feats:
    df[c] = df[c].astype('int')
df = df.replace([np.inf, -np.inf], np.nan)

xgb_params = {
    'objective': 'multi:softprob',
    'num_class': 3,
    'eval_metric': 'mlogloss',
    'n_jobs': -1,
    'seed': 42,
    'enable_categorical': True,
    
    # --- Optuna Best Params ---
    'learning_rate': 0.015,
    'max_depth': 6,
    'subsample': 0.625,
    'colsample_bytree': 0.867,
    'min_child_weight': 4,
    'lambda': 9e-8,
    'alpha': 2e-6,
    # å¢åŠ ä¸€ç‚¹ boost round å› ä¸ºå­¦ä¹ ç‡å˜ä½äº†
    'n_estimators': 2000 
}

xgb_oof = np.zeros((len(df), 3))

for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target'])):
    X_train, y_train = df.iloc[train_idx][features], df.iloc[train_idx]['target']
    X_val, y_val = df.iloc[val_idx][features], df.iloc[val_idx]['target']
    
    # å†æ¬¡ç¡®ä¿åˆ‡åˆ†åçš„æ•°æ®ä¹Ÿæ²¡æœ‰ inf (åŒé‡ä¿é™©)
    X_train = X_train.replace([np.inf, -np.inf], np.nan)
    X_val = X_val.replace([np.inf, -np.inf], np.nan)

    # dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True) 
    # æ³¨æ„ï¼šå¦‚æœ enable_categorical=True æŠ¥é”™ï¼Œå¯ä»¥å°è¯•å»æ‰å®ƒï¼Œ
    # å› ä¸ºå‰é¢å¦‚æœä½ æ²¡æŠŠ category è½¬ intï¼ŒåŒ…å« NaN çš„ category å¯èƒ½ä¼šæœ‰é—®é¢˜
    # å»ºè®®ä¿æŒ enable_categorical=Trueï¼ŒXGBoost æ–°ç‰ˆæ”¯æŒ NaN category
    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
    dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)
    
    model = xgb.train(
        xgb_params, dtrain, 
        num_boost_round=1000,
        evals=[(dval, 'val')],
        early_stopping_rounds=50,
        verbose_eval=False
    )
    
    xgb_oof[val_idx] = model.predict(dval)
    # ä¿å­˜æ¨¡å‹
    model.save_model(f"{OUTPUT_DIR}/xgb_fold{fold}.json")
    print(f"   XGB Fold {fold} LogLoss: {log_loss(y_val, xgb_oof[val_idx]):.5f}")

print(f"ğŸ† Final XGBoost CV Score: {log_loss(df['target'], xgb_oof):.5f}")

from scipy.optimize import minimize

print("\nâš–ï¸ Finding Best Ensemble Weights...")

# å‡†å¤‡ç”¨äºèåˆçš„é¢„æµ‹ç»“æœåˆ—è¡¨
# æ³¨æ„ï¼šè¿™é‡Œè¦ç¡®ä¿é¡ºåºï¼
# 1. LightGBM OOF
# 2. XGBoost OOF
# 3. DeBERTa OOF (ä½œä¸ºæ­£åˆ™é¡¹åŠ å…¥)
deb_keys = [k for k in OOF_PATHS.keys() if 'deberta' in k]

if deb_keys:
    target_key = deb_keys[0] # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ key
    
    # 2. æ„é€ åˆ—ååˆ—è¡¨
    target_cols = [f'{target_key}_a', f'{target_key}_b', f'{target_key}_tie']
    
    # 3. ä» DataFrame ä¸­æå–æ•°æ®
    deberta_oof_probs = df[target_cols].values
    
    print(f"   -> Extracted DeBERTa probs from columns: {target_cols}")
else:
    raise ValueError("âŒ Could not find 'deberta' key in OOF_PATHS configuration!")

predictions_list = [lgb_oof, xgb_oof, deberta_oof_probs]
model_names = ["LightGBM", "XGBoost", "DeBERTa"]

# å¦‚æœä½ æœ‰ Qwenï¼Œè®°å¾—åŠ è¿›å»ï¼š
# qwen_cols = [c for c in df.columns if 'qwen' in c and c.endswith(('_a', '_b', '_tie'))]
# if qwen_cols:
#     qwen_oof_probs = df[qwen_cols].values
#     predictions_list.append(qwen_oof_probs)
#     model_names.append("Qwen")

def log_loss_func(weights):
    final_preds = np.zeros_like(predictions_list[0])
    # å½’ä¸€åŒ–æƒé‡
    weights = np.array(weights) / np.sum(weights)
    
    for i, pred in enumerate(predictions_list):
        final_preds += weights[i] * pred
        
    return log_loss(df['target'], final_preds)

# åˆå§‹æƒé‡å‡åˆ†
init_w = [1/len(predictions_list)] * len(predictions_list)
# çº¦æŸï¼šèŒƒå›´ 0-1ï¼Œå’Œä¸º 1
bounds = [(0, 1)] * len(predictions_list)
constraints = ({'type': 'eq', 'fun': lambda w: 1 - sum(w)})

res = minimize(log_loss_func, init_w, method='SLSQP', bounds=bounds, constraints=constraints)
best_w = res.x / np.sum(res.x)

print("-" * 30)
print(f"ğŸ”¥ Best Ensemble CV Score: {res.fun:.5f}")
print("Weights:")
for name, w in zip(model_names, best_w):
    print(f"  {name}: {w:.4f}")
print("-" * 30)