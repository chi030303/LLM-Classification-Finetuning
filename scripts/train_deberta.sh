#!/bin/bash

# 1. è·¯å¾„ä¸ç¯å¢ƒ
PYTHON_EXEC="/root/autodl-tmp/envs/llm_finetune/bin/python"
PROJECT_ROOT="/root/autodl-tmp/llm_classification_finetuning"
cd "$PROJECT_ROOT" || exit 1

# ç¯å¢ƒå˜é‡ (OFFLINE ä¿æŒå¼€å¯ï¼Œç¡®ä¿ç»å¯¹ä¸è”ç½‘)
export HF_HOME="/root/autodl-tmp/.cache/huggingface"
export HF_HUB_OFFLINE=1
export TOKENIZERS_PARALLELISM=false
export PYTHONUNBUFFERED=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# å®šä¹‰æœ¬åœ°æ¨¡å‹è·¯å¾„ [å…³é”®ä¿®æ”¹]
MODEL_PATH="/root/autodl-tmp/base_models/deberta-v3-large"

echo "ğŸš€ Starting Serial Training on RTX 6000 (Max Performance)..."

# 2. å¾ªç¯è·‘ Fold 0 åˆ° Fold 4 (ä¸²è¡Œ)
for fold in {0..4}
do
    echo "----------------------------------------------------------------"
    echo "â–¶ï¸  Running FOLD $fold"
    echo "----------------------------------------------------------------"
    
    mkdir -p outputs/logs
    LOG_FILE="outputs/logs/deberta_fold${fold}.log"
    
    # 3. è¿è¡Œå‚æ•°ä¼˜åŒ–
    # max_len: 1800 (ä¿æŒ)
    # batch_size: æå‡åˆ° 4 (æ˜¾å­˜å¤Ÿç”¨)
    # grad_acc: é™ä½åˆ° 4 (ä¿æŒæ€»batch=16)
    # num_workers: æå‡åˆ° 8 (å•ä»»åŠ¡ç‹¬å æ›´å¤šCPU)
    
    $PYTHON_EXEC src/train/train_deberta.py \
        --fold $fold \
        --model_name "$MODEL_PATH" \
        --data_path "data/processed/train_with_folds.csv" \
        --output_dir "outputs/models/deberta_v3_large" \
        --max_len 1800 \
        --per_device_train_batch_size 4 \
        --gradient_accumulation_steps 4 \
        --num_train_epochs 3 \
        --learning_rate 5e-6 \
        --gradient_checkpointing \
        --bf16 \
        --dataloader_num_workers 8 \
        > "$LOG_FILE" 2>&1
    
    if [ $? -ne 0 ]; then
        echo "âŒ Fold $fold Failed! Check $LOG_FILE"
        exit 1
    fi
    
    echo "âœ… Fold $fold Completed."
done

echo "ğŸ† All Folds Finished!"