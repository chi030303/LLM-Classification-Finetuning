#!/bin/bash

# 1. Ë∑ØÂæÑ‰∏éÁéØÂ¢É
PYTHON_EXEC="/root/autodl-tmp/envs/llm_finetune/bin/python"
PROJECT_ROOT="/root/autodl-tmp/llm_classification_finetuning"
cd "$PROJECT_ROOT" || exit 1

export HF_HOME="/root/autodl-tmp/.cache/huggingface"
export HF_HUB_OFFLINE=1
export PYTHONUNBUFFERED=1

# --- [ÂÖ≥ÈîÆ] Ë°•Ë∑ë Fold 2 ---
FOLD_ID=2
echo "üöÄ Resuming/Starting Qwen 14B Training for FOLD $FOLD_ID..."

# 2. Ëá™Âä®ÂØªÊâæÊúÄÊñ∞ÁöÑ Checkpoint
OUTPUT_DIR="outputs/models/qwen_14b_fold${FOLD_ID}"
LATEST_CHECKPOINT=$(ls -d ${OUTPUT_DIR}/checkpoint-*/ 2>/dev/null | sort -V | tail -n 1)

RESUME_ARG=""
if [ -d "$LATEST_CHECKPOINT" ]; then
    echo "üîÑ Found checkpoint, resuming from: $LATEST_CHECKPOINT"
    # [ÂÖ≥ÈîÆ] ÊûÑÈÄ†Áª≠ËÆ≠ÂèÇÊï∞
    RESUME_ARG="--resume_from_checkpoint $LATEST_CHECKPOINT"
else
    echo "üöÄ No checkpoint found, starting new run for Fold $FOLD_ID."
fi

# 3. Êó•ÂøóÊñá‰ª∂
mkdir -p outputs/logs
LOG_FILE="outputs/logs/qwen_fold${FOLD_ID}_retry.log"

# 4. ËøêË°åÂëΩ‰ª§ (Âä†ÂÖ•ÊòæÂ≠ò‰ºòÂåñÂèÇÊï∞)
# 96GB ÊòæÂ≠òË∑ë 3072 ÈïøÂ∫¶ÔºåBS=8, GradAcc=2 ÊòØÊØîËæÉÁ®≥ÁöÑÈÖçÁΩÆ
$PYTHON_EXEC src/train/train_qwen.py \
    --fold $FOLD_ID \
    --model_name "/root/autodl-tmp/base_models/Qwen3-14B" \
    --max_len 3072 \
    --learning_rate 1e-4 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 2 \
    --gradient_checkpointing \
    --bf16 \
    $RESUME_ARG \
    > "$LOG_FILE" 2>&1

if [ $? -ne 0 ]; then
    echo "‚ùå Fold $FOLD_ID Failed! Check $LOG_FILE"
else
    echo "‚úÖ Fold $FOLD_ID Completed."
fi